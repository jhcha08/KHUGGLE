{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOQlBf+YJkywhO48XSf1eqz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["필요한 패키지들을 불러옵니다."],"metadata":{"id":"2pxr5ODgpyFb"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"ZL20yQ_clsq8","executionInfo":{"status":"ok","timestamp":1669784366798,"user_tz":-540,"elapsed":3888,"user":{"displayName":"차정훈 (별로장 YouTube)","userId":"17408755466079675051"}}},"outputs":[],"source":["import zipfile\n","\n","import os\n","import os.path\n","from os.path import join\n","from os import listdir\n","\n","import numpy as np\n","import random\n","from collections import OrderedDict\n","import cv2\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torch.utils.data as data\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision.transforms import Compose, ToTensor\n","\n","from math import log10, sqrt\n","import skimage.color as sc"]},{"cell_type":"markdown","source":["대회를 위한 데이터를 다운로드합니다."],"metadata":{"id":"ILuCz70Up4yS"}},{"cell_type":"code","source":["!gdown https://drive.google.com/uc?id=1NJlIDC5gVctEHMqgr_WkxOrjNhvVyNpM\n","!gdown https://drive.google.com/uc?id=1K68hLWgBY0Gj6e3gwuQYspDPeTMgnGqT\n","!gdown https://drive.google.com/uc?id=1Tfo_UzktBtGSpJfcCcTbYXrhOlBIwwQd\n","\n","zip_train = zipfile.ZipFile('/content/khuggle_train.zip')\n","zip_train.extractall('/content/khuggle_train')\n"," \n","zip_train.close()\n","\n","zip_val = zipfile.ZipFile('/content/khuggle_val.zip')\n","zip_val.extractall('/content/khuggle_val')\n"," \n","zip_val.close()\n","\n","zip_test = zipfile.ZipFile('/content/khuggle_test.zip')\n","zip_test.extractall('/content/khuggle_test')\n"," \n","zip_test.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":674},"id":"OBlvDb-HoX1V","executionInfo":{"status":"error","timestamp":1669784379476,"user_tz":-540,"elapsed":9959,"user":{"displayName":"차정훈 (별로장 YouTube)","userId":"17408755466079675051"}},"outputId":"4b162b9c-edd9-424c-b437-d5718f110351"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Access denied with the following error:\n","\n"," \tCannot retrieve the public link of the file. You may need to change\n","\tthe permission to 'Anyone with the link', or have had many accesses. \n","\n","You may still be able to access the file from the browser:\n","\n","\t https://drive.google.com/uc?id=1NJlIDC5gVctEHMqgr_WkxOrjNhvVyNpM \n","\n","Downloading...\n","From: https://drive.google.com/uc?id=1K68hLWgBY0Gj6e3gwuQYspDPeTMgnGqT\n","To: /content/khuggle_test.zip\n","100% 3.19M/3.19M [00:00<00:00, 49.3MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1Tfo_UzktBtGSpJfcCcTbYXrhOlBIwwQd\n","To: /content/khuggle_val.zip\n","100% 5.34M/5.34M [00:00<00:00, 31.0MB/s]\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-c0a6ca2c920a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gdown https://drive.google.com/uc?id=1Tfo_UzktBtGSpJfcCcTbYXrhOlBIwwQd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mzip_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/khuggle_train.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mzip_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/khuggle_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[1;32m   1238\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/khuggle_train.zip'"]}]},{"cell_type":"markdown","source":["데이터셋과 관련된 함수들을 정의합니다.\n","\n"],"metadata":{"id":"rNS3K9hJqA17"}},{"cell_type":"code","source":["def get_patch(*args, patch_size, scale):\n","    ih, iw = args[0].shape[:2]\n","\n","    tp = patch_size  # target patch (HR)\n","    ip = tp // scale  # input patch (LR)\n","\n","    ix = random.randrange(0, iw - ip + 1)\n","    iy = random.randrange(0, ih - ip + 1)\n","    tx, ty = scale * ix, scale * iy\n","\n","    ret = [\n","        args[0][iy:iy + ip, ix:ix + ip, :],\n","        *[a[ty:ty + tp, tx:tx + tp, :] for a in args[1:]]\n","    ] \n","    return ret\n","\n","\n","def set_channel(*args, n_channels=3):\n","    def _set_channel(img):\n","        if img.ndim == 2:\n","            img = np.expand_dims(img, axis=2)\n","\n","        c = img.shape[2]\n","        if n_channels == 1 and c == 3:\n","            img = np.expand_dims(sc.rgb2ycbcr(img)[:, :, 0], 2)\n","        elif n_channels == 3 and c == 1:\n","            img = np.concatenate([img] * n_channels, 2)\n","\n","        return img\n","\n","    return [_set_channel(a) for a in args]\n","\n","\n","def np2Tensor(*args, rgb_range):\n","    def _np2Tensor(img):\n","        np_transpose = np.ascontiguousarray(img.transpose((2, 0, 1)))\n","        tensor = torch.from_numpy(np_transpose).float()\n","        tensor.mul_(rgb_range / 255)\n","\n","        return tensor\n","\n","    return [_np2Tensor(a) for a in args]\n","\n","\n","def augment(*args, hflip=True, rot=True):\n","    hflip = hflip and random.random() < 0.5\n","    vflip = rot and random.random() < 0.5\n","    rot90 = rot and random.random() < 0.5\n","\n","    def _augment(img):\n","        if hflip: img = img[:, ::-1, :]\n","        if vflip: img = img[::-1, :, :]\n","        if rot90: img = img.transpose(1, 0, 2)\n","\n","        return img\n","\n","    return [_augment(a) for a in args]\n","\n","def default_flist_reader(flist):\n","    imlist = []\n","    with open(flist, 'r') as rf:\n","        for line in rf.readlines():\n","            impath = line.strip()\n","            imlist.append(impath)\n","\n","    return imlist\n","\n","\n","IMG_EXTENSIONS = [\n","    '.jpg', '.JPG', '.jpeg', '.JPEG',\n","    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP', '.npy'\n","]\n","\n","\n","def is_image_file(filename):\n","    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n","\n","\n","def make_dataset(dir):\n","    images = []\n","    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n","\n","    for root, _, fnames in sorted(os.walk(dir)):\n","        for fname in fnames:\n","            if is_image_file(fname):\n","                path = os.path.join(root, fname)\n","                images.append(path)\n","\n","    return images\n","\n","\n","def default_loader(path):\n","    return Image.open(path).convert('RGB')\n","\n","\n","class ImageFolder(data.Dataset):\n","\n","    def __init__(self, root, transform=None, return_paths=False,\n","                 loader=default_loader):\n","        imgs = make_dataset(root)\n","        if len(imgs) == 0:\n","            raise(RuntimeError(\"Found 0 images in: \" + root + \"\\n\"\n","                               \"Supported image extensions are: \" +\n","                               \",\".join(IMG_EXTENSIONS)))\n","\n","        self.root = root\n","        self.imgs = imgs\n","        self.transform = transform\n","        self.return_paths = return_paths\n","        self.loader = loader\n","\n","    def __getitem__(self, index):\n","        path = self.imgs[index]\n","        img = self.loader(path)\n","        if self.transform is not None:\n","            img = self.transform(img)\n","        if self.return_paths:\n","            return img, path\n","        else:\n","            return img\n","\n","    def __len__(self):\n","        return len(self.imgs)\n","\n","\n","def is_image_file(filename):\n","    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n","\n","\n","def make_dataset(dir):\n","    images = []\n","    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n","\n","    for root, _, fnames in sorted(os.walk(dir)):\n","        for fname in fnames:\n","            if is_image_file(fname):\n","                path = os.path.join(root, fname)\n","                images.append(path)\n","    return images\n","\n","\n","class Div2k(data.Dataset):\n","    def __init__(self, root, phase, train_size, batch_size, patch_size):\n","        self.scale = 4\n","        self.root = root\n","        self.ext = '.png'   # '.png' or '.npy'(default)\n","        self.train = True if phase == 'train' else False\n","        self.repeat = 1000 // (train_size // batch_size) # 800 = train dataset size\n","        self._set_filesystem(self.root)\n","        self.images_hr, self.images_lr = self._scan()\n","        self.patch_size = patch_size\n","        self.train_size = train_size\n","\n","    def _set_filesystem(self, dir_data):\n","        self.root = dir_data\n","        self.dir_hr = os.path.join(self.root, 'khuggle_DIV2K_hr')\n","        self.dir_lr = os.path.join(self.root, 'khuggle_DIV2K_lr')\n","\n","    def __getitem__(self, idx):\n","        lr, hr = self._load_file(idx)\n","        lr, hr = self._get_patch(lr, hr)\n","        lr, hr = set_channel(lr, hr, n_channels=3)\n","        lr_tensor, hr_tensor = np2Tensor(lr, hr, rgb_range=1)\n","        return lr_tensor, hr_tensor\n","\n","    def __len__(self):\n","        if self.train:\n","            return self.train_size * self.repeat\n","\n","    def _get_index(self, idx):\n","        if self.train:\n","            return idx % self.train_size\n","        else:\n","            return idx\n","\n","    def _get_patch(self, img_in, img_tar):\n","        patch_size = self.patch_size\n","        scale = self.scale\n","        if self.train:\n","            img_in, img_tar = get_patch(\n","                img_in, img_tar, patch_size=patch_size, scale=scale)\n","            img_in, img_tar = augment(img_in, img_tar)\n","        else:\n","            ih, iw = img_in.shape[:2]\n","            img_tar = img_tar[0:ih * scale, 0:iw * scale, :]\n","        return img_in, img_tar\n","\n","    def _scan(self):\n","        list_hr = sorted(make_dataset(self.dir_hr))\n","        list_lr = sorted(make_dataset(self.dir_lr))\n","        return list_hr, list_lr\n","\n","    def _load_file(self, idx):\n","        idx = self._get_index(idx)\n","        if self.ext == '.npy':\n","            lr = npy_loader(self.images_lr[idx])\n","            hr = npy_loader(self.images_hr[idx])\n","        else:\n","            lr = default_loader(self.images_lr[idx])\n","            hr = default_loader(self.images_hr[idx])\n","        return lr, hr\n","\n","\n","class DatasetFromFolderVal(data.Dataset):\n","    def __init__(self, hr_dir, lr_dir, upscale):\n","        super(DatasetFromFolderVal, self).__init__()\n","        self.hr_filenames = sorted([join(hr_dir, x) for x in listdir(hr_dir) if is_image_file(x)])\n","        self.lr_filenames = sorted([join(lr_dir, x) for x in listdir(lr_dir) if is_image_file(x)])\n","        self.upscale = upscale\n","\n","    def __getitem__(self, index):\n","        input = load_image(self.lr_filenames[index])\n","        target = load_image(self.hr_filenames[index])\n","        input = np2tensor()(input)\n","        target = np2tensor()(img_modcrop(target, self.upscale))\n","\n","        return input, target\n","\n","    def __len__(self):\n","        return len(self.lr_filenames)"],"metadata":{"id":"Zo5NavtCmOni"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["성능 측정 및 각종 유틸리티 관련 함수들을 정의합니다."],"metadata":{"id":"Q9X8yq8HqhtR"}},{"cell_type":"code","source":["def compute_psnr(original, compressed):\n","    mse = np.mean((original - compressed) ** 2)\n","    if(mse == 0):  # MSE is zero means no noise is present in the signal .\n","                  # Therefore PSNR have no importance.\n","        return 100\n","    max_pixel = 255.0\n","    psnr = 20 * log10(max_pixel / sqrt(mse))\n","    return psnr\n","\n","\n","def shave(im, border):\n","    border = [border, border]\n","    im = im[border[0]:-border[0], border[1]:-border[1], ...]\n","    return im\n","\n","\n","def modcrop(im, modulo):\n","    sz = im.shape\n","    h = np.int32(sz[0] / modulo) * modulo\n","    w = np.int32(sz[1] / modulo) * modulo\n","    ims = im[0:h, 0:w, ...]\n","    return ims\n","\n","\n","def get_list(path, ext):\n","    return [os.path.join(path, f) for f in os.listdir(path) if f.endswith(ext)]\n","\n","\n","def convert_shape(img):\n","    img = np.transpose((img * 255.0).round(), (1, 2, 0))\n","    img = np.uint8(np.clip(img, 0, 255))\n","    return img\n","\n","\n","def tensor2np(tensor, out_type=np.uint8, min_max=(0, 1)):\n","    tensor = tensor.float().cpu().clamp_(*min_max)\n","    tensor = (tensor - min_max[0]) / (min_max[1] - min_max[0])  # to range [0, 1]\n","    img_np = tensor.numpy()\n","    img_np = np.transpose(img_np, (1, 2, 0))\n","    if out_type == np.uint8:\n","        img_np = (img_np * 255.0).round()\n","\n","    return img_np.astype(out_type)\n","\n","\n","def convert2np(tensor):\n","    return tensor.cpu().mul(255).clamp(0, 255).byte().squeeze().permute(1, 2, 0).numpy()\n","\n","\n","def adjust_learning_rate(optimizer, epoch, step_size, lr_init, gamma):\n","    factor = epoch // step_size\n","    lr = lr_init * (gamma ** factor)\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","\n","def img_modcrop(image, modulo):\n","    sz = image.size\n","    w = np.int32(sz[0] / modulo) * modulo\n","    h = np.int32(sz[1] / modulo) * modulo\n","    out = image.crop((0, 0, w, h))\n","    return out\n","\n","\n","def np2tensor():\n","    return Compose([\n","        ToTensor(),\n","    ])\n","\n","\n","def is_image_file(filename):\n","    return any(filename.endswith(extension) for extension in [\".bmp\", \".png\", \".jpg\"])\n","\n","\n","def load_image(filepath):\n","    return Image.open(filepath).convert('RGB')\n","\n","\n","def default_loader(path):\n","    return cv2.imread(path, cv2.IMREAD_UNCHANGED)[:, :, [2, 1, 0]]\n","\n","\n","def npy_loader(path):\n","    return np.load(path)\n","\n","\n","def load_state_dict(path):\n","    state_dict = torch.load(path)\n","    new_state_dcit = OrderedDict()\n","    for k, v in state_dict.items():\n","        if 'module' in k:\n","            name = k[7:]\n","        else:\n","            name = k\n","        new_state_dcit[name] = v\n","    return "],"metadata":{"id":"EA61VdwZmRt3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이번 대회에서 사용할 초해상화 모델인 IMDN(Information Multi-distillation Network)의 구조를 이루는 함수들을 정의합니다."],"metadata":{"id":"mbrKiaa-qrB4"}},{"cell_type":"code","source":["def conv_layer(in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n","    padding = int((kernel_size - 1) / 2) * dilation\n","    return nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=padding, bias=bias, dilation=dilation,\n","                     groups=groups)\n","\n","\n","def norm(norm_type, nc):\n","    norm_type = norm_type.lower()\n","    if norm_type == 'batch':\n","        layer = nn.BatchNorm2d(nc, affine=True)\n","    elif norm_type == 'instance':\n","        layer = nn.InstanceNorm2d(nc, affine=False)\n","    else:\n","        raise NotImplementedError('normalization layer [{:s}] is not found'.format(norm_type))\n","    return layer\n","\n","\n","def pad(pad_type, padding):\n","    pad_type = pad_type.lower()\n","    if padding == 0:\n","        return None\n","    if pad_type == 'reflect':\n","        layer = nn.ReflectionPad2d(padding)\n","    elif pad_type == 'replicate':\n","        layer = nn.ReplicationPad2d(padding)\n","    else:\n","        raise NotImplementedError('padding layer [{:s}] is not implemented'.format(pad_type))\n","    return layer\n","\n","\n","def get_valid_padding(kernel_size, dilation):\n","    kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)\n","    padding = (kernel_size - 1) // 2\n","    return padding\n","\n","\n","def conv_block(in_nc, out_nc, kernel_size, stride=1, dilation=1, groups=1, bias=True,\n","               pad_type='zero', norm_type=None, act_type='relu'):\n","    padding = get_valid_padding(kernel_size, dilation)\n","    p = pad(pad_type, padding) if pad_type and pad_type != 'zero' else None\n","    padding = padding if pad_type == 'zero' else 0\n","\n","    c = nn.Conv2d(in_nc, out_nc, kernel_size=kernel_size, stride=stride, padding=padding,\n","                  dilation=dilation, bias=bias, groups=groups)\n","    a = activation(act_type) if act_type else None\n","    n = norm(norm_type, out_nc) if norm_type else None\n","    return sequential(p, c, n, a)\n","\n","\n","def activation(act_type, inplace=True, neg_slope=0.05, n_prelu=1):\n","    act_type = act_type.lower()\n","    if act_type == 'relu':\n","        layer = nn.ReLU(inplace)\n","    elif act_type == 'lrelu':\n","        layer = nn.LeakyReLU(neg_slope, inplace)\n","    elif act_type == 'prelu':\n","        layer = nn.PReLU(num_parameters=n_prelu, init=neg_slope)\n","    else:\n","        raise NotImplementedError('activation layer [{:s}] is not found'.format(act_type))\n","    return layer\n","\n","\n","class ShortcutBlock(nn.Module):\n","    def __init__(self, submodule):\n","        super(ShortcutBlock, self).__init__()\n","        self.sub = submodule\n","\n","    def forward(self, x):\n","        output = x + self.sub(x)\n","        return output\n","\n","\n","def mean_channels(F):\n","    assert(F.dim() == 4)\n","    spatial_sum = F.sum(3, keepdim=True).sum(2, keepdim=True)\n","    return spatial_sum / (F.size(2) * F.size(3))\n","\n","\n","def stdv_channels(F):\n","    assert(F.dim() == 4)\n","    F_mean = mean_channels(F)\n","    F_variance = (F - F_mean).pow(2).sum(3, keepdim=True).sum(2, keepdim=True) / (F.size(2) * F.size(3))\n","    return F_variance.pow(0.5)\n","\n","\n","def sequential(*args):\n","    if len(args) == 1:\n","        if isinstance(args[0], OrderedDict):\n","            raise NotImplementedError('sequential does not support OrderedDict input.')\n","        return args[0]\n","    modules = []\n","    for module in args:\n","        if isinstance(module, nn.Sequential):\n","            for submodule in module.children():\n","                modules.append(submodule)\n","        elif isinstance(module, nn.Module):\n","            modules.append(module)\n","    return nn.Sequential(*modules)\n","\n","\n","class CCALayer(nn.Module):\n","    def __init__(self, channel, reduction=16):\n","        super(CCALayer, self).__init__()\n","\n","        self.contrast = stdv_channels\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.conv_du = nn.Sequential(\n","            nn.Conv2d(channel, channel // reduction, 1, padding=0, bias=True),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(channel // reduction, channel, 1, padding=0, bias=True),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        y = self.contrast(x) + self.avg_pool(x)\n","        y = self.conv_du(y)\n","        return x * y\n","\n","\n","class IMDModule(nn.Module):\n","    def __init__(self, in_channels, distillation_rate=0.25):\n","        super(IMDModule, self).__init__()\n","        self.distilled_channels = int(in_channels * distillation_rate)\n","        self.remaining_channels = int(in_channels - self.distilled_channels)\n","        self.c1 = conv_layer(in_channels, in_channels, 3)\n","        self.c2 = conv_layer(self.remaining_channels, in_channels, 3)\n","        self.c3 = conv_layer(self.remaining_channels, in_channels, 3)\n","        self.c4 = conv_layer(self.remaining_channels, self.distilled_channels, 3)\n","        self.act = activation('lrelu', neg_slope=0.05)\n","        self.c5 = conv_layer(in_channels, in_channels, 1)\n","        self.cca = CCALayer(self.distilled_channels * 4)\n","\n","    def forward(self, input):\n","        out_c1 = self.act(self.c1(input))\n","        distilled_c1, remaining_c1 = torch.split(out_c1, (self.distilled_channels, self.remaining_channels), dim=1)\n","        out_c2 = self.act(self.c2(remaining_c1))\n","        distilled_c2, remaining_c2 = torch.split(out_c2, (self.distilled_channels, self.remaining_channels), dim=1)\n","        out_c3 = self.act(self.c3(remaining_c2))\n","        distilled_c3, remaining_c3 = torch.split(out_c3, (self.distilled_channels, self.remaining_channels), dim=1)\n","        out_c4 = self.c4(remaining_c3)\n","        out = torch.cat([distilled_c1, distilled_c2, distilled_c3, out_c4], dim=1)\n","        out_fused = self.c5(self.cca(out)) + input\n","        return out_fused\n","    \n","def pixelshuffle_block(in_channels, out_channels, upscale_factor=2, kernel_size=3, stride=1):\n","    conv = conv_layer(in_channels, out_channels * (upscale_factor ** 2), kernel_size, stride)\n","    pixel_shuffle = nn.PixelShuffle(upscale_factor)\n","    return sequential(conv, pixel_shuffle)"],"metadata":{"id":"gJ_DRrdqmUrR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["IMDN 네트워크를 정의합니다. 자세한 내용은 논문 (https://arxiv.org/pdf/1909.11856v1.pdf) 를 참고해주세요.\n"],"metadata":{"id":"NqaNRkulr5fh"}},{"cell_type":"code","source":["class IMDN(nn.Module):\n","    def __init__(self, in_nc=3, nf=64, num_modules=6, out_nc=3, upscale=4):\n","        super(IMDN, self).__init__()\n","\n","        self.fea_conv = conv_layer(in_nc, nf, kernel_size=3)\n","\n","        # IMDBs\n","        self.IMDB1 = IMDModule(in_channels=nf)\n","        self.IMDB2 = IMDModule(in_channels=nf)\n","        self.IMDB3 = IMDModule(in_channels=nf)\n","        self.IMDB4 = IMDModule(in_channels=nf)\n","        self.IMDB5 = IMDModule(in_channels=nf)\n","        self.IMDB6 = IMDModule(in_channels=nf)\n","        self.c = conv_block(nf * num_modules, nf, kernel_size=1, act_type='lrelu')\n","\n","        self.LR_conv = conv_layer(nf, nf, kernel_size=3)\n","\n","        upsample_block = pixelshuffle_block\n","        self.upsampler = upsample_block(nf, out_nc, upscale_factor=upscale)\n","\n","\n","    def forward(self, input):\n","        out_fea = self.fea_conv(input)\n","        out_B1 = self.IMDB1(out_fea)\n","        out_B2 = self.IMDB2(out_B1)\n","        out_B3 = self.IMDB3(out_B2)\n","        out_B4 = self.IMDB4(out_B3)\n","        out_B5 = self.IMDB5(out_B4)\n","        out_B6 = self.IMDB6(out_B5)\n","\n","        out_B = self.c(torch.cat([out_B1, out_B2, out_B3, out_B4, out_B5, out_B6], dim=1))\n","        out_lr = self.LR_conv(out_B) + out_fea\n","        output = self.upsampler(out_lr)\n","        return output"],"metadata":{"id":"n8apH1RkmV9y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["모델 훈련을 시작합니다."],"metadata":{"id":"iF9wmibQsbd6"}},{"cell_type":"code","source":["!pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu100/torch_nightly.html"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":233},"id":"mtotM8xmCKXm","executionInfo":{"status":"ok","timestamp":1669098757946,"user_tz":-540,"elapsed":74995,"user":{"displayName":"차정훈 (별로장 YouTube)","userId":"17408755466079675051"}},"outputId":"93c04045-cb27-4c5d-b185-c73560c54d82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.pytorch.org/whl/nightly/cu100/torch_nightly.html\n","Collecting torch_nightly\n","  Downloading https://download.pytorch.org/whl/nightly/cu100/torch_nightly-1.2.0.dev20190805-cp37-cp37m-linux_x86_64.whl (790.0 MB)\n","\u001b[K     |████████████████████████████████| 790.0 MB 21 kB/s \n","\u001b[?25hInstalling collected packages: torch-nightly\n","Successfully installed torch-nightly-1.2.0.dev20190805\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["torch"]}}},"metadata":{}}]},{"cell_type":"code","source":["print('''\n","\n","@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n","@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@   @@@@@   .@@.  #@@@@@@&   @@@*  *@@@@@@   &@@@@@@@       .@@@@@@@@        @@@@*  /@@@@@@@@         @@@@@@@@@@@@@@@@@@\n","@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@   @@@.   @@@@*  /@@@@@@@   @@@#  .@@@@@@   #@@@@    /@@@@% @@@@@    .@@@@@ .@@@#   @@@@@@@@   @@@@@@@@@@@@@@@@@@@@@@@@@\n","@@@@@@@@@@@@@@@@@@@@@@@@@@@@@   @%   @@@@@@#   @@@@@@@   @@@@   @@@@@@/  ,@@&   @@@@@@@@@@@@@   &@@@@@@@@@@@@@   @@@@@@@@   &@@@@@@@@@@@@@@@@@@@@@@@@@\n","@@@@@@@@@@@@@@@@@@@@@@@@@@@@      ,@@@@@@@@             @@@@   @@@@@@&   @@,  .@@@@      @@#   @@@@      /@@@   @@@@@@@@         @@@@@@@@@@@@@@@@@@@@@\n","@@@@@@@@@@@@@@@@@@@@@@@@@@@   &@   .@@@@@@   @@@@@@@   &@@@   @@@@@@&   @@&   @@@@@@@   %@@   &@@@@@@#   @@@   @@@@@@@@(  .@@@@@@@@@@@@@@@@@@@@@@@@@@@\n","@@@@@@@@@@@@@@@@@@@@@@@@@@,  *@@@   .@@@@   @@@@@@@.  (@@@@   #@@@&   #@@@@    @@@@@*  *@@@    %@@@@&   @@@   @@@@@@@@&   @@@@@@@@@@@@@@@@@@@@@@@@@@@@\n","@@@@@@@@@@@@@@@@@@@@@@@@@%   @@@@@    @@   &@@@@@@(  .@@@@@&       *@@@@@@@@(        *@@@@@@&         @@@@         *@@         @@@@@@@@@@@@@@@@@@@@@@@\n","@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n","@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n","@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n","@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n","@@@@@@@@          @@@@@&          @@@@@@@.         @@@@@@           @@@@@@@@@@@@@.  (@@@@@@@&   @@@@@@,  /@@@@@,        #@@*  ,@@@@   ,@@@   @@@@*  *@\n","@@@@@   ,@@@@@@@@@@@@   ,@@@@@@    @@@.   @@@@@@(   @@@@   @@@@@@    @@@@@@@@@@@(  .@@@@@@@@   @@@@@@#   @@@.   &@@@@@@@@@&   @@.   @@@@@,  &@@@%  ,@@\n","@@@   #@@@@@@@@@@@@.  ,@@@@@@@@,  .@&   @@@@@@@@@   @@@   @@@@@@@(   @@@@@@@@@@&   @@@@@@@@   @@@@@@&   @@#   @@@@@@@@@@@&   #   @@@@@@@@  @@@@@  .@@@\n","@@   %@@@%      @@.   @@@@@@@@&   @@   @@@@@@@@@   @@@   @@@@@@@&   @@@@@@@@@@@   @@@@@@@@   @@@@@@@   @@(   @@@@@@@@@@@@   *   @@@@@@@@  (@@@@   @@@@\n","@&   &@@@@@@   @@@   /@@@@@@@   /@@.   @@@@@@@/   @@@   %@@@@@@   *@@@@@@@@@@@   @@@@@@@@,  .@@@@@&   @@@    @@@@@@@@@@@   @@.   @@@@@@@@%@@@@#@@@@@@@\n","@@            @@@@%           &@@@@@           *@@@@*           @@@@@@@@@@@@@         &@@*          @@@@@%          @@@   @@@@.   &@@@   @@@@   &@@@@@\n","@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n","\n","''')\n","\n","\n","torch.backends.cudnn.benchmark = True\n","\n","# random seed\n","seed = 0#random.randint(1, 10000)\n","print(\"Ramdom Seed: \", seed)\n","random.seed(seed)\n","torch.manual_seed(seed)\n","\n","cuda = True\n","device = torch.device('cuda' if cuda else 'cpu')\n","\n","step_size = 200\n","lr = 2e-4\n","gamma = 0.5\n","\n","nEpochs = 1000\n","\n","print(\"===> Loading datasets\")\n","\n","trainset = Div2k(root='/content/khuggle_train', phase='train', train_size=800, batch_size=16, patch_size=192)\n","testset = DatasetFromFolderVal(hr_dir=\"/content/khuggle_val/Set14_hr/\",\n","                               lr_dir=\"/content/khuggle_val/Set14_lr/\", upscale=4)\n","\n","training_data_loader = DataLoader(dataset=trainset, num_workers=2, batch_size=16, shuffle=True, pin_memory=True, drop_last=True)\n","testing_data_loader = DataLoader(dataset=testset, num_workers=2, batch_size=1, shuffle=False)\n","\n","print(\"===> Building models\")\n","\n","model = IMDN(upscale=4)\n","l1_criterion = nn.L1Loss()\n","\n","print(\"===> Setting GPU\")\n","\n","if cuda:\n","    model = model.to(device)\n","    l1_criterion = l1_criterion.to(device)\n","\n","# pre-trained 된 모델이 있을 경우 None을 주석 처리하고, pre-trained 된 모델의 경로를 작성합니다.\n","\n","pretrained = None # '/models/imdn.pth'\n","\n","if pretrained:\n","\n","    if os.path.isfile(pretrained):\n","        print(\"===> loading models '{}'\".format(pretrained))\n","        checkpoint = torch.load(pretrained)\n","        new_state_dcit = OrderedDict()\n","        for k, v in checkpoint.items():\n","            if 'module' in k:\n","                name = k[7:]\n","            else:\n","                name = k\n","            new_state_dcit[name] = v\n","        model_dict = model.state_dict()\n","        pretrained_dict = {k: v for k, v in new_state_dcit.items() if k in model_dict}\n","\n","        for k, v in model_dict.items():\n","            if k not in pretrained_dict:\n","                print(k)\n","        model.load_state_dict(pretrained_dict, strict=True)\n","\n","    else:\n","        print(\"===> no models found at '{}'\".format(pretrained))\n","\n","print(\"===> Setting Optimizer\")\n","\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","def train(epoch):\n","    model.train()\n","    adjust_learning_rate(optimizer, epoch, step_size, lr, gamma)\n","\n","    print('epoch =', epoch, 'lr = ', optimizer.param_groups[0]['lr'])\n","\n","    for iteration, (lr_tensor, hr_tensor) in enumerate(training_data_loader, 1):\n","\n","        if cuda:\n","            lr_tensor = lr_tensor.to(device)  # ranges from [0, 1]\n","            hr_tensor = hr_tensor.to(device)  # ranges from [0, 1]\n","\n","        optimizer.zero_grad()\n","        sr_tensor = model(lr_tensor)\n","        loss_l1 = l1_criterion(sr_tensor, hr_tensor)\n","        loss_sr = loss_l1\n","\n","        loss_sr.backward()\n","        optimizer.step()\n","\n","        if iteration % 100 == 0:\n","            print(\"===> Epoch[{}]({}/{}): Loss_l1: {:.5f}\".format(epoch, iteration, len(training_data_loader), loss_l1.item()))\n","\n","\n","def valid():\n","    model.eval()\n","\n","    avg_psnr = 0\n","    for batch in testing_data_loader:\n","        lr_tensor, hr_tensor = batch[0], batch[1]\n","        if cuda:\n","            lr_tensor = lr_tensor.to(device)\n","            hr_tensor = hr_tensor.to(device)\n","        \n","        with torch.no_grad():\n","            pre = model(lr_tensor)\n","        sr_img = tensor2np(pre.detach()[0])\n","        gt_img = tensor2np(hr_tensor.detach()[0])\n","        crop_size = 4 # scale\n","        cropped_sr_img = shave(sr_img, crop_size)\n","        cropped_gt_img = shave(gt_img, crop_size)\n","        \n","        im_label = cropped_gt_img\n","        im_pre = cropped_sr_img\n","\n","        avg_psnr += compute_psnr(im_pre, im_label)\n","    \n","    print(\"===> Valid. psnr: {:.4f}\".format(avg_psnr / len(testing_data_loader)))\n","\n","\n","def save_checkpoint(epoch):\n","    model_folder = \"checkpoint/\"\n","    model_out_path = model_folder + \"epoch_{}.pth\".format(epoch)\n","    if not os.path.exists(model_folder):\n","        os.makedirs(model_folder)\n","    torch.save(model.state_dict(), model_out_path)\n","    \n","    print(\"===> Checkpoint saved to {}\".format(model_out_path))\n","\n","def print_network(net):\n","    num_params = 0\n","    for param in net.parameters():\n","        num_params += param.numel()\n","    \n","    print(net)\n","    print('Total number of parameters: %d' % num_params)\n","\n","\n","print(\"===> Training\")\n","print_network(model)\n","\n","for epoch in range(1, nEpochs + 1):\n","    valid()\n","    train(epoch)\n","    save_checkpoint(epoch)"],"metadata":{"id":"4BhAID7AmXV2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cuda = True\n","device = torch.device('cuda' if cuda else 'cpu')\n","\n","test_lr_folder = '/home/hallasan/junghun/EDSR-PyTorch/dataset/benchmark/Set5/LR_bicubic/X4/'\n","output_folder = './khuggle_output/'\n","\n","lrs = os.listdir(test_lr_folder[:-1])\n","\n","filelist = get_list(test_lr_folder, ext='.png')\n","\n","# 추론에 사용할 모델 가중치 불러오기\n","checkpoint = '/home/hallasan/junghun/checkpoint/epoch_4.pth'\n","\n","model = IMDN(upscale=4)\n","model.load_state_dict(torch.load(checkpoint), strict=True)\n","\n","start = torch.cuda.Event(enable_timing=True)\n","end = torch.cuda.Event(enable_timing=True)\n","\n","for i, imname in enumerate(filelist):\n","    print(imname)\n","    im_l = cv2.imread(imname, cv2.IMREAD_COLOR)[:, :, [2, 1, 0]]  # BGR to RGB\n","    if len(im_l.shape) < 3:\n","        im_l = im_l[..., np.newaxis]\n","        im_l = np.concatenate([im_l] * 3, 2)\n","    im_input = im_l / 255.0\n","    im_input = np.transpose(im_input, (2, 0, 1))\n","    im_input = im_input[np.newaxis, ...]\n","    im_input = torch.from_numpy(im_input).float()\n","\n","    if cuda:\n","        model = model.to(device)\n","        im_input = im_input.to(device)\n","\n","    with torch.no_grad():\n","        start.record()\n","        out = model(im_input)\n","        end.record()\n","        torch.cuda.synchronize()\n","\n","    out_img = tensor2np(out.detach()[0])\n","\n","    if not os.path.exists(output_folder):\n","        os.makedirs(output_folder)\n","\n","    cv2.imwrite(output_folder + lrs[i][:-4], out_img[:, :, [2, 1, 0]])"],"metadata":{"id":"K9F9WCz1mZec"},"execution_count":null,"outputs":[]}]}